<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-05T16:13:56-03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ben Conrad</title><subtitle>Ben's Blog
</subtitle><entry><title type="html">Using a Packing Discriminator in Generative Adversarial Networks</title><link href="http://localhost:4000/2019/07/19/using-a-packing-discriminator-in-generative-adversarial-networks.html" rel="alternate" type="text/html" title="Using a Packing Discriminator in Generative Adversarial Networks" /><published>2019-07-19T14:09:00-03:00</published><updated>2019-07-19T14:09:00-03:00</updated><id>http://localhost:4000/2019/07/19/using-a-packing-discriminator-in-generative-adversarial-networks</id><content type="html" xml:base="http://localhost:4000/2019/07/19/using-a-packing-discriminator-in-generative-adversarial-networks.html">&lt;style&gt;
td {
  font-size: 18px
}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 60%;
}
&lt;/style&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The following was originally written in December 2018 as a final project for my undergraduate machine learning course. Additions and clarifications have been made while transfering the report over from LaTex.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Generative Adversarial Networks (GANs) [&lt;a href=&quot;#Goodfellow14&quot;&gt;Goodfellow14&lt;/a&gt;] are a family of generative models that have had recent success in generating samples from complex distributions. GANs have been used to produce realistic text, numerical and image samples effectively however they generally encounter a common issue where the model produces samples with little diversity. This problem is referred to as &lt;em&gt;mode collapse&lt;/em&gt; and a significant amount of work has been put into finding techniques to alleviate this issue. One of these proposed techniques is a simple extension to the discriminator architecture called &lt;em&gt;packing&lt;/em&gt; [&lt;a href=&quot;#Lin18&quot;&gt;Lin18&lt;/a&gt;] where the network is trained to validate against multiple samples jointly.&lt;/p&gt;

&lt;p&gt;In this report we be performing an empirical analysis on packing to better understand how it works in GAN training. We test networks with and without a packing discriminator on synthetic datasets where mode collapse can be easily be monitored and distribution statistics can be changed to observe how effective packing is under different settings.&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;
&lt;h3 id=&quot;mode-collapse-in-gans&quot;&gt;Mode Collapse in GANs:&lt;/h3&gt;

&lt;p&gt;Mode collapse is a phenomenon in GANs where the generator produces samples that lack the same diversity as the target distribution. More specifically, mode collapse is when the generator’s learned distribution assigns a significantly smaller probability density around the region of a mode compared to that of the target distribution [&lt;a href=&quot;#Lin18&quot;&gt;Lin18&lt;/a&gt;]. Mode collapse occurs when the generator becomes so overconfident in certain samples fooling the discriminator that instead of exploring the rest of the data manifold for undiscovered modes, the generator continues to produce similar samples to minimize its loss function.&lt;/p&gt;

&lt;p&gt;An example of mode collapse happens while training on the MNIST dataset which has the 10 digits
(0-9) as its modes. Using a standard GAN architecture, the model often fails to generate all of the digits, only discovering a few of the distribution’s modes no matter how long the network trains.&lt;/p&gt;

&lt;div style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/assets/mnist_collapse.PNG&quot; /&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot; style=&quot;font-size:15px&quot;&gt; &lt;b&gt;Figure 0: A GAN encountering mode collapse (bottom) vs one that is not (top) [&lt;a href=&quot;#Cho18&quot;&gt;Cho18&lt;/a&gt;]&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Several approaches have been proposed to deal with mode collapse. These approaches include label
smoothing, different loss functions, mixing multiple GANs together and using batch statistics during
validation. While many of these techniques have shown to be effective, there is little understanding
as to why certain techniques work and which are the most suitable for different cases.&lt;/p&gt;

&lt;h3 id=&quot;packing&quot;&gt;Packing:&lt;/h3&gt;

&lt;p&gt;Packing is an extension to the standard GAN architecture where the discriminator labels multiple
samples jointly to a single real/fake label. The same general network architecture and loss function
are maintained from the standard GAN however the discriminator is modified from a network that maps a single input $x$ to a binary label, $D:x \rightarrow \{ 0,1 \}$, into a packing discriminator which maps $m$ inputs  $x_1, x_2, …, x_m$ to a single joint binary label, $D:x_1, x_2, …, x_m \rightarrow \{ 0,1 \}$. We refer to $m$ as the &lt;em&gt;degree of packing&lt;/em&gt; and the $m$ samples are drawn independently from the real distribution $P$ and the generator distribution $Q$ while training the discriminator.&lt;/p&gt;

&lt;p&gt;In a standard GAN, the network can be thought as learning a distribution $Q$ which minimizes the distance between itself and the real distribution $P$, $\min d(P,Q)$. When using packing, the discriminator is given samples from the product distribution of degree $m$ which changes the optimization problem to $
\min d(P^m, Q^m)$. Exposing the discriminator to the product distribution allows for it to better detect the presence of diversity (or lack there of) in generated examples enforcing the generator to explore a wider area of the data manifold and avoid missing modes.&lt;/p&gt;

&lt;p&gt;Packing introduces little added computation, going from $\mathcal{O}(wrg^2)$ per minibatch update in a standard GAN to $\mathcal{O}((w+m)rg^2)$ in a GAN using packing of degree $m$ where $w$ is the number of fully connected layers, $g$ is the number of nodes per layer and $r$ is the minibatch size [&lt;a href=&quot;#Lin18&quot;&gt;Lin18&lt;/a&gt;].&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&quot;setup&quot;&gt;Setup&lt;/h3&gt;

&lt;p&gt;To analyze the effects of using a packing discriminator, we compare networks with and without packing across different variations of our baseline dataset. The dataset is a 2-dimensional multivariate Gaussian distribution $\mathcal{N}(\mu, \Sigma)$ with 25 means at $(-4+2i, -4+2j)$ for $i,j \in \{0,1,2,3,4\}$, each with $\Sigma=0.0025 \cdot \mathcal{I}$ (seen in the leftmost plot of &lt;a href=&quot;#Fig1&quot;&gt;Figure 1&lt;/a&gt;). Experiments are done on variations of the baseline dataset with different variances, mode concentrations and arrangements.&lt;/p&gt;

&lt;p&gt;All of the networks use the same generator and discriminator architectures which can be seen in &lt;a href=&quot;#Table1&quot;&gt;Table 1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Generator&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Discriminator&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$z \in \mathcal{R}^{2} \sim \mathcal{N}(0, \mathcal{I})$&lt;/td&gt;
      &lt;td&gt;$x_1, x_2, … , x_m \in \mathcal{R}^{2}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$Dense(2 \rightarrow 400)$, BN, ReLU&lt;/td&gt;
      &lt;td&gt;$Linear(2 \cdot m \rightarrow 200)$, LinearMaxOut(5)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$Dense(400 \rightarrow 400)$, BN, ReLU&lt;/td&gt;
      &lt;td&gt;$Dense(200 \rightarrow 200)$, LinearMaxOut(5)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$Dense(400 \rightarrow 400)$, BN, ReLU&lt;/td&gt;
      &lt;td&gt;$Dense(200 \rightarrow 200)$, LinearMaxOut(5)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$Dense(400 \rightarrow 400)$, BN, ReLU&lt;/td&gt;
      &lt;td&gt;$Dense(200 \rightarrow 1)$, Sigmoid&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$Dense(400 \rightarrow 2)$, BN, Linear&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot; style=&quot;font-size:15px&quot;&gt; &lt;b&gt;Table 1: Generator and discriminator architectures&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The discriminator uses LinearMaxout [&lt;a href=&quot;#Goodfellow13&quot;&gt;Goodfellow13&lt;/a&gt;] with 5 maxout units as its activation function. The generator and discriminator both use the standard GAN loss function from [&lt;a href=&quot;#Goodfellow14&quot;&gt;Goodfellow14&lt;/a&gt;]. The synthetic training dataset has 100,000 samples that the networks are trained on for 100 epochs using Adam [&lt;a href=&quot;#Kingma14&quot;&gt;Kingma14&lt;/a&gt;] with equal updates on the generator and discriminator. All other parameters can be found in &lt;a href=&quot;#Table2&quot;&gt;Table 2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;lr = 0.0001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;beta1 = 0.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;beta2 = 0.999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;minibatch size = 100&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p align=&quot;center&quot; style=&quot;font-size:15px&quot;&gt; &lt;b&gt;Table 2: Additonal hyper-parameters&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;During the experiments we monitor how many modes the generator learns over time. For multivariate Gaussian distributions, a mode exists at each of the distribution’s means and we consider that a mode is lost if no sample within 3 standard deviations of the center of a mode is generated during testing. Other metrics measured are the number of epochs it takes the network to learn all the modes, the proportion of generated samples within 3 standard deviations of a mode (% high-quality samples) and the Jensen-Shannon divergence (JSD) between the target and learned distributions. All metrics are calculated from 2500 samples generated after each epoch.&lt;/p&gt;

&lt;p&gt;The dataset, hyper-parameters and metrics closely follow those from the 2D-grid experiment in [&lt;a href=&quot;#Lin18&quot;&gt;Lin18&lt;/a&gt;].&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;The first set of experiments examine the effects of packing on data with different levels of noise applied to it which is simulated by increasing the variance of our baseline target distribution. On the noise-free baseline distribution (&lt;a href=&quot;#Fig1&quot;&gt;Figure 1&lt;/a&gt; and &lt;a href=&quot;#Table3&quot;&gt;Table 3&lt;/a&gt;), the network is only capable of generating 19 of the target distribution’s modes when using a standard discriminator however by adding packing the network learns all 25 modes at $m=3$.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Fig1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/0025-fig.png&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Figure 1: Samples from a 25 mode distribution with $\Sigma=0.0025 \cdot \mathcal{I}$&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table3&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;/assets/0025-table.png&quot; alt=&quot;drawing&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Table 3: Results from Figure 1&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;When noise is added to the distribution the severity of mode collapse decreases with the standard GAN, recovering 23 modes when $\Sigma=0.01 \cdot \mathcal{I}$ (&lt;a href=&quot;#Fig2&quot;&gt;Figure 2&lt;/a&gt; and &lt;a href=&quot;#Table4&quot;&gt;Table 4&lt;/a&gt;) and all 25 when $\Sigma=0.1 \cdot \mathcal{I}$ (&lt;a href=&quot;#Fig3&quot;&gt;Figure 3&lt;/a&gt; and &lt;a href=&quot;#Table5&quot;&gt;Table 5&lt;/a&gt;). Applying noise to the inputs of the discriminator is a technique that has been explored before [&lt;a href=&quot;#Salimans16&quot;&gt;Salimans16&lt;/a&gt;, &lt;a href=&quot;#Arjovsky17&quot;&gt;Arjovsky17&lt;/a&gt;] and is understood to help smooth the target distribution’s probability mass. In our experiments, applying noise increase the area in the data manifold where the training data lies making the discriminator less strict and allowing the generator to explore without being penalized as severely.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Fig2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-fig.png&quot; alt=&quot;Figure 2&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Figure 2: Samples from a 25 mode distribution with $\Sigma=0.01 \cdot \mathcal{I}$&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/01-table.png&quot; alt=&quot;drawing&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Table 4: Results from Figure 2&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Fig3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/1-fig.png&quot; alt=&quot;Figure 3&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Figure 3: Samples from a 25 mode distribution with $\Sigma=0.1 \cdot \mathcal{I}$&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/1-table.png&quot; alt=&quot;drawing&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Table 5: Results from Figure 3&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;The other experiments looked at how the complexity of the distribution impacts the effectiveness of
packing. On a low complexity distribution with 9 modes and $\Sigma = 0.0025 \cdot \mathcal{I}$
(&lt;a href=&quot;#Fig4&quot;&gt;Figure 4&lt;/a&gt; and &lt;a href=&quot;#Table6&quot;&gt;Table 6&lt;/a&gt;), the standard GAN can only produce 8 modes after 100
epochs however when packing with $m=2$ is added, remaining the network quickly discovers all 9 in just 3 epochs. This observation shows that the non-packing network quickly converges to 8 of the modes and then stops exploring to prioritize improving the quality of the samples for the modes it has already discovered. When packing is added this bottleneck is bypassed immediately and the network discovers the final center mode.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Fig4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/9modes-fig.png&quot; alt=&quot;Figure 4&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Figure 4: Samples from a 9 mode distribution with $\Sigma=0.0025 \cdot \mathcal{I}$&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/9modes-table.png&quot; alt=&quot;drawing&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Table 6: Results from Figure 4&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;On a high complexity distribution with 81 modes and $\Sigma = 0.0025 \cdot \mathcal{I}$ (&lt;a href=&quot;#Fig5&quot;&gt;Figure
5&lt;/a&gt; and &lt;a href=&quot;#Table7&quot;&gt;Table 7&lt;/a&gt;), none of the three networks are able to discover all modes. No
packing and packing with $m=2$ discover 59 and 58 modes respectively however we do see $m=3$ significantly outperforming the others discovering 69 of the 81 modes. The architecture used in the experiment is clearly not deep enough to adequately learn this complex distribution however we can still observe from the experiments that adding packing can reduce the degree of mode collapse even in shallow networks.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Fig5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/81modes-fig.png&quot; alt=&quot;Figure 5&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Figure 5: Samples from a 81 mode distribution with $\Sigma=0.0025 \cdot \mathcal{I}$&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/81modes-table.png&quot; alt=&quot;drawing&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Table 7: Results from Figure 5&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;To see if the distances between modes impacts how the network explores the data manifold we perform
the last experiment on a baseline distribution with randomly spaced modes (&lt;a href=&quot;#Fig6&quot;&gt;Figure 6&lt;/a&gt; and
&lt;a href=&quot;#Table8&quot;&gt;Table 8&lt;/a&gt;). All three networks were unable to discover all 23 modes (two pairs of modes
combined during the random shuffling) with them each producing
the same 20-21. From the plots in &lt;a href=&quot;#Fig6&quot;&gt;Figure 6&lt;/a&gt;, we can see that each network failed to discover
the same 3 modes in the center-left region of the data manifold. These modes have a relatively
larger distance to its neighbours compared to the rest which caused the network to not explore far 
 away from the modes it had already discovered and find the remaining modes. This observation shows
 that GANs can become biased in its learning, focusing on exploring regions of the manifold that
 have a high likelihood of generating samples that will fool the discriminator and fails to explore
 the low likelihood region, where new modes can be discovered, even when packing is being used. How the target distribution is modeled highly influences the generator’s susceptibility to mode collapse and could be important to consider in future work.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Fig6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rand-fig.png&quot; alt=&quot;Figure 6&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Figure 6: Samples from a 23 randomly spaced mode distribution with $\Sigma=0.0025 \cdot \mathcal{I}$&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Table8&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/rand-table-new.png&quot; alt=&quot;drawing&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;padding-top: 0px; font-size:15px&quot;&gt; &lt;b&gt;Table 8: Results from Figure 6&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Across all experiments, a couple of trends can be seen about packing:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. As the packing degree $m$ increases, the number of epochs it takes the network to discover all
the target distribution’s modes also increases.&lt;/strong&gt; This is best seen in the experiment with $\Sigma =
0.1 \cdot \mathcal{I}$ (&lt;a href=&quot;#Fig3&quot;&gt;Figure 3&lt;/a&gt; and &lt;a href=&quot;#Table5&quot;&gt;Table 5&lt;/a&gt;) where the network not using
packing takes 16 epochs to discover all 25 modes while using packing with $m=2$ takes 27 epochs and
$m=3$ takes 62. The reason behind this can be explained by how increasing $m$ causes the
discriminator to become stricter about the diversity of the inputs it is validating. This leads to
the generator being given less information and as a result taking longer to learn. This is the
biggest downfall of packing since GAN training is already very slow and other techniques like adding noise do not hinder the model’s convergence rate to the same degree.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Packing is shown to generate a distribution with a smaller JSD compared that from a standard
GAN.&lt;/strong&gt; In some cases, this can be explained simply by the model discovering more modes which cause
a lower JSD by definition however this fact can be also observed in experiments where both the
packing and non-packing networks discover the same number of modes ($\Sigma = 0.1 \cdot \mathcal{I}$
[&lt;a href=&quot;#Fig3&quot;&gt;Figure 3&lt;/a&gt; and &lt;a href=&quot;#Table5&quot;&gt;Table 5&lt;/a&gt;] and randomly spaced modes [&lt;a href=&quot;#Fig6&quot;&gt;Figure 6&lt;/a&gt; and &lt;a href=&quot;#Table8&quot;&gt;Table
8&lt;/a&gt;]). Since minimizing the JSD is equivalent to finding the optimal discriminator
[&lt;a href=&quot;#Goodfellow14&quot;&gt;Goodfellow14&lt;/a&gt;], we can say that packing produces a theoretically better model. On
the other side, packing has shown to decrease the proportion of high-quality samples compared to
that of the standard GAN during these experiments. This quality difference can be explained by how
packing causes the network to take more time to discover all the modes (which it is capable of
discovering) leading to less time for the generator to prioritize producing high-quality samples. If
we were to continue training beyond the 100 epochs done in the experiments, the packing networks should be capable of eventually achieving the same % high-quality samples as the standard GAN.&lt;/p&gt;

&lt;p&gt;Using JSD as a model quality metric has also been found to have issues. In image generation, a small JSD has been shown to not always be correlated with visually superior samples and is the reason why models are typically evaluated using metrics like Inception score [&lt;a href=&quot;#Salimans16&quot;&gt;Salimans16&lt;/a&gt;] or FID [&lt;a href=&quot;#Heusel17&quot;&gt;Heusel17&lt;/a&gt;]. The advantage of a lower JSD is therefore not as significant as we may have hoped for in image generation but in other domains like numerical data, this may be an important property for a model to have.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Packing is a simple extension to the standard discriminator architecture that empirically shows to
reduce mode collapse in many cases. The technique typically produces a distribution that includes
more modes from the target distribution with a lower JS divergence than a non-packing network
however other techniques like using noise can also alleviate mode collapse while taking less time to
train. Packing is an interesting technique that may be beneficial in certain cases but has issues
which hold it back from being widely usable.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;Goodfellow14&quot;&gt;&lt;/a&gt;[Goodfellow14] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … &amp;amp; Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Lin18&quot;&gt;&lt;/a&gt;[Lin18] Lin, Z., Khetan, A., Fanti, G., &amp;amp; Oh, S. (2018). Pacgan: The power of two samples in generative adversarial networks. In Advances in Neural Information Processing Systems (pp. 1498-1507).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Goodfellow13&quot;&gt;&lt;/a&gt;[Goodfellow13] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., &amp;amp; Bengio, Y. (2013). Maxout networks. arXiv preprint arXiv:1302.4389.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Kingma14&quot;&gt;&lt;/a&gt;[Kingma14] Kingma, D. P., &amp;amp; Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Arjovsky17&quot;&gt;&lt;/a&gt;[Arjovsky17] Arjovsky, M., &amp;amp; Bottou, L. (2017). Towards Principled Methods for Training Generative Adversarial Networks. arXiv preprint arXiv:1701.04862.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Salimans16&quot;&gt;&lt;/a&gt;[Salimans16] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp;amp; Chen, X. (2016). Improved techniques for training gans. In Advances in neural information processing systems (pp. 2234-2242).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Heusel17&quot;&gt;&lt;/a&gt;[Heusel17] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., &amp;amp; Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems (pp. 6626-6637).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Cho18&quot;&gt;&lt;/a&gt;[Cho18] Cho, J. (2018, April 04). CycleGAN : Image Translation with GAN (4). Retrieved from http://tmmse.xyz/2018/04/02/image-translation-with-gan-4/&lt;/p&gt;</content><author><name></name></author><summary type="html">The following was originally written in December 2018 as a final project for my undergraduate machine learning course. Additions and clarifications have been made while transfering the report over from LaTex.</summary></entry><entry><title type="html">Belief Contractions on Large Ontologies with Minimal Knowledge Loss</title><link href="http://localhost:4000/2019/07/05/belief-contractions-on-large-ontologies-with-minimal-knowledge-loss.html" rel="alternate" type="text/html" title="Belief Contractions on Large Ontologies with Minimal Knowledge Loss" /><published>2019-07-05T14:54:00-03:00</published><updated>2019-07-05T14:54:00-03:00</updated><id>http://localhost:4000/2019/07/05/belief-contractions-on-large-ontologies-with-minimal-knowledge-loss</id><content type="html" xml:base="http://localhost:4000/2019/07/05/belief-contractions-on-large-ontologies-with-minimal-knowledge-loss.html">&lt;style&gt;
td {
  font-size: 18px
}
&lt;/style&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The following was work I did during my NSERC Undergraduate Research Assistant position at Simon Fraser University during Summer 2018. The report assumes the reader has prior knowledge about knowledge representation particularly descriptive logics.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The field of knowledge representation deals with finding efficient methods to represent, store and perform inference on large collections of data. When dealing with large knowledge bases, a user may require to remove a fact that was previously believed to be true however has been rendered false after the addition of new information. What makes this operation difficult is that simply removing the belief  (represented as a formal logic axiom) is often not enough since the combinations of many other beliefs in the knowledge base can also infer the same false fact resulting in no knowledge actually being removed. Various contraction methods working on different formal logics have been proposed which ensures that a belief is completely forgotten by removing multiple axioms from a knowledge base. In [&lt;a href=&quot;#Dawood17&quot;&gt;Dawood17&lt;/a&gt;], a kernel contraction algorithm was constructed for $\mathcal{EL}$ TBox. The contraction is performed by removing a minimum set of axioms which infer the belief and using a heuristic to select the a prefered set when multiple minimum sets exist. One of these heuristics is &lt;em&gt;Specificity&lt;/em&gt; which weighs axioms by their generality within the domain. We will be expanding upon the Specificity heuristic to create a total preorder relation that orders axioms based on the amount of epistemic loss that they cause when removed from a TBox. The &lt;strong&gt;Hierarchical Total Preorder&lt;/strong&gt;, will work on $\mathcal{EL^{++}}$ TBoxes and will be shown how it can be implemented into the kernel contraction algorithm.&lt;/p&gt;

&lt;h2 id=&quot;mathcalel-description-logic&quot;&gt;$\mathcal{EL^{++}}$ Description Logic&lt;/h2&gt;

&lt;p&gt;Description Logics (DLs) [&lt;a href=&quot;#Baader07&quot;&gt;Baader07&lt;/a&gt;] are a family of logics used to model relationships between entities in a domain. DLs consist of three types of entities, concepts which represent sets of individuals, roles which describe relationships between individuals and singleton individuals from a domain. A DL knowledge base is composed of two parts, the ABox containing extensional knowledge and the TBox containing intensional knowledge. The ABox states assertions about individuals using concepts and roles such as $Doctor(Betty)$ and $brotherOf(Tim, Jill)$. The TBox contains subsumption axioms that describe relationships between concepts and roles such $Dog \sqsubseteq Animal$ and $brotherOf \sqsubseteq parentOf$. Many DLs exist with varying expressibility and reasoning complexity. The language that we will be using is $\mathcal{EL^{++}}$.&lt;/p&gt;

&lt;p&gt;$\mathcal{EL^{++}}$ [&lt;a href=&quot;#Baader05&quot;&gt;Baader05&lt;/a&gt;], an extension of $\mathcal{EL}$, is a lightweight DL that has limited expressibility but boasts polynomial time reasoning and is used on large ontologies like SNOMED CT. The table below outlines the syntax of the language.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Syntax&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;Semantics&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;top&lt;/td&gt;
      &lt;td&gt;$\top$&lt;/td&gt;
      &lt;td&gt;$\Delta^{I}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;bottom&lt;/td&gt;
      &lt;td&gt;$\bot$&lt;/td&gt;
      &lt;td&gt;$\emptyset$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;nominal&lt;/td&gt;
      &lt;td&gt;{$a$}&lt;/td&gt;
      &lt;td&gt;{$a^I$}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;conjunction&lt;/td&gt;
      &lt;td&gt;$C \sqcap D$&lt;/td&gt;
      &lt;td&gt;$C^I\cap$$D^I$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;existential restriction&lt;/td&gt;
      &lt;td&gt;$\exists r.C$&lt;/td&gt;
      &lt;td&gt;{$x \in \Delta^I$ $|$$\exists$ $y \in \Delta^I$: &lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;                               $(x,y) \in$ $r^I$ $\land$ $y$ $\in$ $C^I$}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;concrete domain&lt;/td&gt;
      &lt;td&gt;$p(f_1, …\; ,f_k)$ for $p \in$ $R$&lt;/td&gt;
      &lt;td&gt;{$x \in$ $\Delta^I$ $|$$\exists y_1, …\; , y_k \in$ $\Delta^{D_j}$: &lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;                               $f_i^I(x) = y_i$ for $1 \le i \le k \land$ &lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;                   $(y_1, …\; y_k)$ $\in p^{D_j}$}&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GCI&lt;/td&gt;
      &lt;td&gt;$C \sqsubseteq D$&lt;/td&gt;
      &lt;td&gt;$C^I \sqsubseteq D^I$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RI&lt;/td&gt;
      &lt;td&gt;$r_1 \circ … \circ r_k \sqsubseteq r$&lt;/td&gt;
      &lt;td&gt;$r_1^I \circ … \circ r_k^I \sqsubseteq r^I$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;An $\mathcal{EL^{++}}$ TBox is a finite and consistent set of GCIs and RIs. We refer to the left hand side expression as the &lt;em&gt;sub-concept&lt;/em&gt; or &lt;em&gt;sub-role&lt;/em&gt; and the right hand side expression as the &lt;em&gt;super-concept&lt;/em&gt; or &lt;em&gt;super-role&lt;/em&gt; for GCIs and RIs respectively.&lt;/p&gt;

&lt;h2 id=&quot;belief-change&quot;&gt;Belief Change&lt;/h2&gt;

&lt;p&gt;The most prominently used construction of belief change is the AGM framework [&lt;a href=&quot;#Alchourron85&quot;&gt;Alchourron85&lt;/a&gt;]. The framework models an agent’s state of knowledge with a belief set which is a closed under logical implication set of sentences. Belief sets state exactly what the agent currently perceives as true. There are three belief change operations for modifying these sets:&lt;/p&gt;

&lt;p&gt;       &lt;strong&gt;Expansion:&lt;/strong&gt; Adding a new belief to a belief set.&lt;/p&gt;

&lt;p&gt;       &lt;strong&gt;Contraction:&lt;/strong&gt; Removing a belief from a belief set.&lt;/p&gt;

&lt;p&gt;       &lt;strong&gt;Revision:&lt;/strong&gt; Adding a new belief which may create an inconsistent belief set that requires other beliefs to be removed.&lt;/p&gt;

&lt;p&gt;The operation we will be focusing on is contraction.&lt;/p&gt;

&lt;p&gt;While AGM describes contractions using belief sets, [&lt;a href=&quot;#Hansson93&quot;&gt;Hansson93&lt;/a&gt;] describes a different approach using belief bases. Belief bases are sets of beliefs not closed under logical implication which better models what an agent with finite memory would store and are equivalent to DL TBoxes.&lt;/p&gt;

&lt;p&gt;Two methods of belief base contractions are regularly used, partial meet contractions and kernel contractions. Partial meet contractions [&lt;a href=&quot;#Alchourron85&quot;&gt;Alchourron85&lt;/a&gt;] are done by using remainder sets, maximal subsets of a belief base $K$ that do not entail the axiom we wish to contract, $\alpha$. The contracted belief base is the intersection of a select set of remainder sets. Kernels [&lt;a href=&quot;#Hansson94&quot;&gt;Hansson94&lt;/a&gt;] are minimal subsets of $K$ that entail $\alpha$. To perform a kernel contraction of $\alpha$ we select an axiom from each kernel and remove them from the belief base. Kernel contraction is the method that we will be considering from now on and is denoted by $K \div \alpha$.&lt;/p&gt;

&lt;p&gt;The following five postulates [&lt;a href=&quot;#Hansson93&quot;&gt;Hansson93&lt;/a&gt;] are used to capture the definition of a belief base kernel contraction:&lt;/p&gt;

&lt;p&gt;      &lt;strong&gt;1) Success&lt;/strong&gt;: If $\nvdash \alpha$ then $K\div\alpha \nvdash \alpha$&lt;/p&gt;

&lt;p&gt;      &lt;strong&gt;2) Inclusion&lt;/strong&gt;: $K\div \alpha \subseteq K$&lt;/p&gt;

&lt;p&gt;      &lt;strong&gt;3) Core retainment&lt;/strong&gt;: If $\beta \in K$ and $\beta \notin K\div\alpha$     then there is a set $K’ \subseteq K$ such that $K’ \nvdash \alpha$ but $K’ \cup \beta \vdash \alpha$&lt;/p&gt;

&lt;p&gt;      &lt;strong&gt;4) Uniformity&lt;/strong&gt;: If for every $K’ \subseteq K$ we have$K’ \vdash \alpha$ iff $K’ \vdash \beta$ then $K \div \alpha = K \div \beta$&lt;/p&gt;

&lt;p&gt;      &lt;strong&gt;5) Relative closure&lt;/strong&gt;: $K \cap Cn(K\div\alpha) \subseteq K\div \alpha$&lt;/p&gt;

&lt;p&gt;If a function satisfies the first four postulates then it is a kernel contraction and if all five hold then it is a smooth kernel
contraction.&lt;/p&gt;

&lt;p&gt;The kernel contraction algorithm that we will be working was introduced in [&lt;a href=&quot;#Dawood17&quot;&gt;Dawood17&lt;/a&gt;]. The algorithm calculates all $\alpha$-kernels, kernels that entail $\alpha$, in $K$ using the axiom pinpointing algorithm [&lt;a href=&quot;#Baader08&quot;&gt;Baader08&lt;/a&gt;]. We denote the set of $\alpha$-kernels in belief base $K$ with $K \perp \alpha$. An incision function then selects axioms from each kernel to remove from the belief base. The set of axioms chosen by the incision function is called the &lt;em&gt;drop set&lt;/em&gt;, $\sigma(K \perp\alpha)$. Since we prefer to remove as few axioms as possible, the incision function selects a minimum drop set. The calculation for minimum drop sets is equivalent to the minimum hitting set problem [&lt;a href=&quot;#Garey79&quot;&gt;Garey79&lt;/a&gt;, &lt;a href=&quot;#Dawood17&quot;&gt;Dawood17&lt;/a&gt;] therefore a hitting set algorithm is used to find drop sets. Once a minimum hitting set is selected for the drop set, the axioms are removed to form the contracted belief base, $K \div \alpha$.&lt;/p&gt;

&lt;h2 id=&quot;hierarchical-total-preorder&quot;&gt;Hierarchical Total Preorder&lt;/h2&gt;

&lt;p&gt;Since an $\mathcal{EL^{++}}$ TBox is equivalent to a belief base we can use the kernel contraction $T\div\alpha$ on some $\mathcal{EL^{++}}$ TBox $T$ and axiom $\alpha$. Once the kernels are calculated and the minimum hitting sets are found we typically have multiple equal sized sets to choose as the drop set. Aside from simply removing as few axioms as possible, we ideally want to achieve the contraction with as minimal knowledge loss to the TBox as possible. Expanding on the specificity heuristic from [&lt;a href=&quot;#Dawood17&quot;&gt;Dawood17&lt;/a&gt;] and exploiting the axiom hierarchy found in $\mathcal{EL^{++}}$, we can define a total preorder binary relation which can order axioms by their importance within the TBox to help make our decision.&lt;/p&gt;

&lt;p&gt;The hierarchical preorder relation, $\le_{HP}$, is based off the concept of an epistemic entrenchment [&lt;a href=&quot;#Gardenfors88&quot;&gt;Gardenfors88&lt;/a&gt;], $\le_{EE}$. An epistemic entrenchment is a total preorder over the axioms of a belief set that represents the relative epistemic loss caused by removing each axiom. The relation $\alpha \le_{EE} \beta$ states that $\beta$ is equally or more entrenched in the knowledge base as $\alpha$ and therefore during contractions we would prefer to remove $\alpha$ over $\beta$. An epistemic entrenchment is defined by five postulates, transitivity, dominance, conjunctiveness, minimality and maximality. These postulates capture the definition of epistemic loss in standard logics however not all postulates can be applied to description logics.&lt;/p&gt;

&lt;p&gt;For the hierarchical preorder, we will form a new set of postulates to formulate a preorder that still uses the metric of epistemic loss to order the axioms but can applied on $\mathcal{EL^{++}}$ TBoxes. In $\mathcal{EL^{++}}$ we can measure epistemic loss as the number of entailments related to the most general expression that are lost. For example, in the TBox $\; T =${$A \sqsubseteq B,\; B \sqsubseteq C, \; C \sqsubseteq D$}, removing $C \sqsubseteq D$ results in losing the entailments $A \sqsubseteq D, \; B \sqsubseteq D$ and $C \sqsubseteq D$, however removing $A \sqsubseteq B$ only loses the entailment $A \sqsubseteq D$. Since removing $A \sqsubseteq B$ causes less epistemic loss we have $A \sqsubseteq B \le_{HP} C \sqsubseteq D$.&lt;/p&gt;

&lt;p&gt;Before we formulate the postulates we need to first define some terminology that will help us describe the subsumption hierarchy of $\mathcal{EL^{++}}$ axioms.&lt;/p&gt;

&lt;h3 id=&quot;def-1-connected-axioms&quot;&gt;Def 1 Connected Axioms:&lt;/h3&gt;

&lt;p&gt;For some TBox $T$ and axioms {$A\sqsubseteq B,\; C\sqsubseteq D$} $\in T$, where $A,B,C,D$ are either all concepts, existential restrictions and conjunctions (GCI) or all roles (RI). If either:&lt;/p&gt;

&lt;p&gt;       $\bullet$ {$A\sqsubseteq B, \; C\sqsubseteq D$}$ \models A\sqsubseteq D$&lt;/p&gt;

&lt;p&gt;       $\bullet$ {$A\sqsubseteq B, \; C\sqsubseteq D, \;\mathcal{S}$}$ \models A\sqsubseteq D$, where $\mathcal{S} \subseteq T$ are &lt;em&gt;support axioms&lt;/em&gt;, and no subset of {$A\sqsubseteq B,\; C\sqsubseteq D, \;\mathcal{S}$} entails $A\sqsubseteq D$&lt;/p&gt;

&lt;p&gt;then $A\sqsubseteq B$ is connected with $C\sqsubseteq D$, $A\sqsubseteq B  \mapsto C\sqsubseteq D$. We refer to $A\sqsubseteq B$ as a $\textit{LHS connecting axiom}$ of $C\sqsubseteq D$ and $C\sqsubseteq D$ as a $\textit{RHS connecting axiom}$ of $A\sqsubseteq B$.&lt;/p&gt;

&lt;h3 id=&quot;def-2-subsumption-path&quot;&gt;Def 2 Subsumption Path:&lt;/h3&gt;

&lt;p&gt;For some TBox $T$, axioms $\alpha$ and $\beta$ are on the same $\textit{subsumption path}$ if there exists a sequence of axioms {$x_1, x_2, … , x_n$}$\in T$ for $n\geq1$, where both:&lt;/p&gt;

&lt;p&gt;       $\bullet$$x_i \mapsto x_{i+1}$ for all $1 \leq i \leq n-1$.&lt;/p&gt;

&lt;p&gt;       $\bullet$$x_1 = \alpha$ and $x_n = \beta$.&lt;/p&gt;

&lt;p&gt;{$x_1, x_2, … , x_n$} is a $\textit{subsumption path}$ of $\alpha$ and $\beta$.&lt;/p&gt;

&lt;p&gt;We now define the four postulates that we will be following while constructing the hierarchical preorder:&lt;/p&gt;

&lt;p&gt;       1) &lt;strong&gt;Transitivity:&lt;/strong&gt; If $\alpha\le_{HP}\beta$ and     $\beta \le_{HP}\delta$, then $\alpha \le_{HP}\delta$.&lt;/p&gt;

&lt;p&gt;       2) &lt;strong&gt;Totality:&lt;/strong&gt; For all $\alpha, \beta$, $\alpha \le_{HP} \beta$ or     $\beta \le_{HP} \alpha$.&lt;/p&gt;

&lt;p&gt;       3) &lt;strong&gt;Minimality:&lt;/strong&gt; If belief base $T$ is consistent, then     $\alpha\notin T$ iff $\alpha \le_{HP}\beta$ for all $\beta$.&lt;/p&gt;

&lt;p&gt;       4) &lt;strong&gt;Hierarchical:&lt;/strong&gt; If $\alpha \mapsto \beta$ and {$\alpha, \beta$}$ \in T$ for some belief base $T$, then
    $\alpha \le_{HP}\beta$.&lt;/p&gt;

&lt;p&gt;Transitivity and totality are the two properties a total preorder must follow. The hierarchical postulate is a new postulate that captures the connection between subsumption hierarchies and epistemic loss in description logics. For example, if we have $\alpha \mapsto \beta$ then$\beta$ is higher in the subsumption hierarchy than $\alpha$ which means removing it will cause more entailments to be lost from the TBox, therefore $\beta$ causes more epistemic loss than $\alpha$.&lt;/p&gt;

&lt;h2 id=&quot;hierarchical-weighting-function&quot;&gt;Hierarchical Weighting Function&lt;/h2&gt;

&lt;p&gt;When given a hierarchical preorder relation like $\alpha \le_{HP} \beta$, we must determine which of the axioms causes
the greater amount of epistemic loss when removed. To measure this, we will weights axioms based on their position within the TBox using the &lt;em&gt;hierarchical weighting function&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The function takes an axiom $\alpha$ and first confirms if $\alpha \in T$ (if not we assign $weight(\alpha) = -1$) and then calculates $weight(\alpha)$ by going through the following 4 phases.&lt;/p&gt;

&lt;h2 id=&quot;subsumption-hierarchy-phase&quot;&gt;Subsumption Hierarchy Phase&lt;/h2&gt;

&lt;p&gt;The initial phase weighs $\alpha$ based off its placement within the TBox’s subsumption hierarchy. This is calculated by using the set of LHS connecting axioms of $\alpha$, $LHS(\alpha)$. To find the LHS connecting axioms that appear from using support axioms we first calculate the indirect children of the existential restrictions in the TBox.&lt;/p&gt;

&lt;h3 id=&quot;def-3-indirect-child&quot;&gt;Def 3 Indirect Child:&lt;/h3&gt;

&lt;p&gt;Given $A$ is a concept, existential restriction or a conjunction of concepts and existential restrictions, $B$ and $C$ are concepts and $r$ and $s$ are roles:&lt;/p&gt;

&lt;p&gt;       $\bullet$ If {$\exists r.C \sqsubseteq A, \;B\sqsubseteq C$} $\in T$, then $children’(\exists r.C) = children(\exists r.C)\cup${$\exists r.B$}&lt;/p&gt;

&lt;p&gt;       $\bullet$ If {$\exists r.C \sqsubseteq A, \;s\sqsubseteq r$} $\in T$, then $children’(\exists r.C) = children(\exists r.C) \cup ${$\exists s.C$}&lt;/p&gt;

&lt;p&gt;       $\bullet$ If {$\exists r.C \sqsubseteq A, \;B\sqsubseteq C,\; s\sqsubseteq r$} $\in T$, then $children’(\exists r.C) = children(\exists r.C) \cup$ {$\exists r.B,\; \exists s.C,\; \exists s.B$}&lt;/p&gt;

&lt;p&gt;We can use these indirect children to find all LHS connecting axioms for each axiom by using the rule:&lt;/p&gt;

&lt;p&gt;       $\bullet$$A\sqsubseteq B \in LHS(C\sqsubseteq D)$ if $B = C$ or $B \in children’(C)$&lt;/p&gt;

&lt;p&gt;The subsumption hierarchy weighting procedure can now be executed as follows:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Subsumption Hierarchy Weighting Procedure:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;       $\bullet$ If $LHS(\alpha) = \emptyset$ then $weight(\alpha)=0$.&lt;/p&gt;

&lt;p&gt;       $\bullet$ If $LHS(\alpha) \ne \emptyset$ then $weight(\alpha) = i+1$ where $i$ is the maximum subsumption hierarchy weight among all axioms in $LHS(\alpha)$.&lt;/p&gt;

&lt;p&gt;Since cycles are allowed to occur in the TBox we have an anti-cycling check for the recursive step in the above procedure. While keeping a list of all previously visited axioms in the current recursive stack, if we try to get $weight(\beta)$ and $\beta$ is already in the visited axiom list we do not consider its weight at the current recursion level.&lt;/p&gt;

&lt;p&gt;An example of the subsumption hierarchy weightings procedure is shown in the following:&lt;/p&gt;

&lt;h3 id=&quot;example-1&quot;&gt;Example 1:&lt;/h3&gt;

&lt;h5 id=&quot;tbox&quot;&gt;TBox:&lt;/h5&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
  $A \sqsubseteq B $  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $B \sqsubseteq C$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $C \sqsubseteq \exists r.D$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $\exists p.E  \sqsubseteq F$ &lt;br /&gt; $A \sqsubseteq \exists p.E$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $D \sqsubseteq E$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;$r \sqsubseteq s$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $s \sqsubseteq p$
&lt;/p&gt;

&lt;h5 id=&quot;lhs-connecting-axioms&quot;&gt;LHS Connecting Axioms:&lt;/h5&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
  $LHS(A \sqsubseteq B)=$ {} &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $LHS(B \sqsubseteq C)=${$A \sqsubseteq B$}&lt;br /&gt;  $LHS(C \sqsubseteq \exists r.D)=${$B \sqsubseteq C$}   &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $LHS(\exists p.E  \sqsubseteq F)=${$C \sqsubseteq \exists r.D, A \sqsubseteq \exists p.E$} &lt;br /&gt; $LHS(A \sqsubseteq \exists p.E) =${} &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $LHS(D \sqsubseteq E)=${} &lt;br /&gt; $LHS(r \sqsubseteq s)=${} &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $LHS(s \sqsubseteq p)=${$r \sqsubseteq s$}
&lt;/p&gt;

&lt;h5 id=&quot;weights&quot;&gt;Weights:&lt;/h5&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
  $weight(A \sqsubseteq B)=0$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(B \sqsubseteq C)=1$ &lt;br /&gt; $weight(C \sqsubseteq \exists r.D)=2$  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(\exists p.E  \sqsubseteq F)=3$ &lt;br /&gt; $weight(A \sqsubseteq \exists p.E)=0$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(D \sqsubseteq E)=0$ &lt;br /&gt; $weight(r \sqsubseteq s)=0$  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(s \sqsubseteq p)=1$ 
&lt;/p&gt;

&lt;h2 id=&quot;support-axiom-phase&quot;&gt;Support Axiom Phase&lt;/h2&gt;

&lt;p&gt;An issue with subsumption hierarchy weights is that support axioms are under-weighted. If we consider Example 1, we have $C \sqsubseteq \exists r.D \mapsto \exists p.E  \sqsubseteq F$ because of the support axioms $S =${$r \sqsubseteq s,\; s \sqsubseteq p,\; D \sqsubseteq E$}. With the axioms of S we have $T \models  C \sqsubseteq F$ however if we remove any of these axioms this would not hold. When we perform a contraction of $A \sqsubseteq E$ on TBox T we get 2 kernels:&lt;/p&gt;

&lt;p&gt;       1) $K_1 =$ {$A \sqsubseteq B,\; B \sqsubseteq C,\;  C \sqsubseteq \exists r.D,\; \exists p.E  \sqsubseteq F,\; D \sqsubseteq E,\; r \sqsubseteq s,\; s \sqsubseteq p$}&lt;/p&gt;

&lt;p&gt;       2) $K_2 = ${$A \sqsubseteq \exists p.E,\; \exists p.E  \sqsubseteq F$}&lt;/p&gt;

&lt;p&gt;Removing the lowest weighted axiom in $K_2$ we simply choose $A \sqsubseteq \exists p.E$, however $K_1$ has 3 axioms with a weight of 0 to choose from, $A \sqsubseteq B,\; r \sqsubseteq s$, and $D \sqsubseteq E$. The issue is removing $A \sqsubseteq B$ preserves $T \models C \sqsubseteq F$, while removing either $r \sqsubseteq s$ or $D \sqsubseteq E$ does not because its causes  $C \sqsubseteq \exists r.D \mapsto\mkern-16mu\not\;\;  \exists p.E  \sqsubseteq F$. Since removing these two axioms causes a greater amount epistemic loss to the TBox we need to adjust their weights to reflect this.&lt;/p&gt;

&lt;p&gt;The support axiom weighting phase makes adjustments by matching the support axioms’ sub-concept/role with axioms that have existential restrictions with the same concept/role in their super-concept.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Support Axiom Weighting Procedure:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given $A,B$ are concepts and $r,s$ are roles:&lt;/p&gt;

&lt;p&gt;       $\bullet$ For axioms in the form $A \sqsubseteq B$ with $weight(A \sqsubseteq B) = i$:&lt;/p&gt;

&lt;p&gt;             $\bullet$ If $X \sqsubseteq \exists r.A \in T$ with $weight(X \sqsubseteq \exists r.A) = j$ and $j \ge i$, then $weight(A \sqsubseteq B) = j$.&lt;/p&gt;

&lt;p&gt;       $\bullet$ For axioms in the form $r \sqsubseteq s$ with $weight(r \sqsubseteq s) = i$:&lt;/p&gt;

&lt;p&gt;             $\bullet$ If $X \sqsubseteq \exists r.A \in T$ with $weight(X \sqsubseteq \exists r.A) = j$ and $j \ge i$, then $weight(r \sqsubseteq s) = j$.&lt;/p&gt;

&lt;p&gt;Continuing with Example 1, applying the support axiom weighting procedure results in the following:&lt;/p&gt;

&lt;h3 id=&quot;example-1-cont&quot;&gt;Example 1 cont.&lt;/h3&gt;

&lt;h5 id=&quot;support-axiom-adjustments&quot;&gt;Support Axiom Adjustments:&lt;/h5&gt;

&lt;p&gt;       $\bullet$ Since {$C \sqsubseteq \exists r.D ,\; r \sqsubseteq s$} $\in T$, $weight(C \sqsubseteq \exists r.D) = 2$ and $weight(r \sqsubseteq s) = 0$, set $weight(r \sqsubseteq s) = 2$.&lt;/p&gt;

&lt;p&gt;       $\bullet$ Since {$C \sqsubseteq \exists r.D ,\; D \sqsubseteq E$} $\in T$, $weight(C \sqsubseteq \exists r.D) = 2$ and $weight(D \sqsubseteq E) = 0$, set $weight(D \sqsubseteq E) = 2$.&lt;/p&gt;

&lt;h5 id=&quot;weights-1&quot;&gt;Weights:&lt;/h5&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
  $weight(A \sqsubseteq B)=0$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(B \sqsubseteq C)=1$ &lt;br /&gt; $weight(C \sqsubseteq \exists r.D)=2$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(\exists p.E  \sqsubseteq F)=3$ &lt;br /&gt; $weight(A \sqsubseteq \exists p.E)=0$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(D \sqsubseteq E)=2$ &lt;br /&gt; $weight(r \sqsubseteq s)=2$  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(s \sqsubseteq p)=1$ 
&lt;/p&gt;

&lt;h2 id=&quot;cycle-adjustment-phase&quot;&gt;Cycle Adjustment Phase:&lt;/h2&gt;

&lt;p&gt;Next is an optional phase to deal with cyclic TBoxes. Say we have the TBox:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
	$A \sqsubseteq B$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $B \sqsubseteq C$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $C \sqsubseteq A$
&lt;/p&gt;

&lt;p&gt;The weights assigned in the subsumption hierarchy phase varies depending on the order the axioms are processed. For example, if we start with $B \sqsubseteq C$ we would gets the weights:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
	$weight(A \sqsubseteq B) = 1$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(B \sqsubseteq C) = 2$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(C \sqsubseteq A) = 0$ 
&lt;/p&gt;

&lt;p&gt;The anti-cycling check prevents the procedure from entering an infinite loop however the hierarchical postulate is broken because $weight(B \sqsubseteq C) &amp;gt;  weight(C \sqsubseteq A)$. Another problem is that the current weights state that removing $C \sqsubseteq A$ causes the least amount of epistemic loss however all of the axioms in the loop cause the same amount of loss. To fix both these issues, the cycle adjustment procedure identifies loops and increases all of the loop’s axioms to the maximum weight among these axioms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cycle Adjustment Procedure:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;       $\bullet$ If $\alpha$ is in a cycle comprised of the set of axioms $\ell \subseteq T$, then $weight(\beta) = i$ where $i = max(weight(\beta))$ for all $\beta \in \ell$.&lt;/p&gt;

&lt;p&gt;Applying this procedure on the above TBox gives us the weights:&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;
  $weight(A \sqsubseteq B) = 2$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(B \sqsubseteq C) = 2$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(C \sqsubseteq A) = 2$ 
&lt;/p&gt;

&lt;h2 id=&quot;offset-adjustment-phase&quot;&gt;Offset Adjustment Phase&lt;/h2&gt;

&lt;p&gt;The support axiom phase adjusts support axioms’ weights to better reflect their potential epistemic loss within the TBox, however these adjustments can break the hierarchical postulate. In Example 1’s TBox, we currently have $weight(r\sqsubseteq s) = 2$ and $weight(s \sqsubseteq p) = 1$ however since $r \sqsubseteq s \mapsto s \sqsubseteq p$ we require $weight(r \sqsubseteq s) \leq weight(s \sqsubseteq p )$. The offset adjustment procedure fixes this by checking that all of the axioms have a larger weight than their LHS connecting axioms and increases the axiom’s weight when this does not hold.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Offset Adjustment Procedure:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;       $\bullet$ If $LHS(\alpha) = \emptyset$:&lt;/p&gt;

&lt;p&gt;             $\bullet$ $o_{\alpha} = 0$&lt;/p&gt;

&lt;p&gt;       $\bullet$ Else, For all $\beta \in LHS(\alpha)$:&lt;/p&gt;

&lt;p&gt;             $\bullet$ If $weight(\beta) &amp;gt; weight(\alpha)$, then $o_{\alpha} = o_{\beta} + weight(\beta) - weight(\alpha) +1$&lt;/p&gt;

&lt;p&gt;             $\bullet$ Else, $o_{\alpha} = o_{\beta}$&lt;/p&gt;

&lt;p&gt;       $\bullet$ $weight(\alpha) = weight(\alpha) + o_{\alpha}$&lt;/p&gt;

&lt;p&gt;Like the subsumption hierarchy weighting procedure, the offset adjustment procedure also contains the same anti-cycling check during the recursive step of calculating $o_{\beta}$ to prevent infinite loops.&lt;/p&gt;

&lt;p&gt;Finishing Example 1, we go through the offset adjustment phase and get the following:&lt;/p&gt;

&lt;h3 id=&quot;example-1-cont-1&quot;&gt;Example 1 cont.&lt;/h3&gt;

&lt;h5 id=&quot;offset-adjustments&quot;&gt;Offset Adjustments:&lt;/h5&gt;

&lt;p&gt;       $\bullet$ Since $weight(r \sqsubseteq s) &amp;gt; weight(s \sqsubseteq p )$:&lt;br /&gt;            - $o_{s \sqsubseteq p} = o_{r \sqsubseteq s} + weight(r \sqsubseteq s) - weight(s \sqsubseteq p) +1 = 0 + 2 - 1 + 1 = 2$&lt;br /&gt;            - $weight(s \sqsubseteq p) = 1 + 2 = 3$.&lt;/p&gt;

&lt;h5 id=&quot;weights-2&quot;&gt;Weights:&lt;/h5&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;
  $weight(A \sqsubseteq B)=0$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(B \sqsubseteq C)=1$ &lt;br /&gt; $weight(C \sqsubseteq \exists r.D)=2$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(\exists p.E  \sqsubseteq F)=3$ &lt;br /&gt; $weight(A \sqsubseteq \exists p.E)=0$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(D \sqsubseteq E)=2$ &lt;br /&gt; $weight(r \sqsubseteq s)=2$ &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; $weight(s \sqsubseteq p)=3$ 
&lt;/p&gt;

&lt;h2 id=&quot;using-hierarchical-weighting-function-in-le_hp&quot;&gt;Using Hierarchical Weighting Function in $\le_{HP}$&lt;/h2&gt;

&lt;p&gt;With the hierarchical weighting function we can now implement $\le_{HP}$ to solve hierarchical preorder relations using the following rule:&lt;/p&gt;

&lt;p&gt;       $\alpha \le_{HP} \beta$ iff $weight(\alpha) \le weight(\beta)$&lt;/p&gt;

&lt;p&gt;Algorithm 1 outlines the relation validity checking and weight calculation processes. Here we are assuming that the TBox T stores pairs of axioms and their weights.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/alg1.png&quot; alt=&quot;Algorithm 1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;postulate-proofs&quot;&gt;Postulate Proofs&lt;/h2&gt;

&lt;p&gt;We will now prove that the hierarchical preorder relation follows the postulates previously introduced.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; The hierarchical preorder binary relation $\le_{HP}$ satisfies the transitivity, totality, minimality and hierarchical postulates when applied to an EL++ TBox.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Given TBox $T$ and axioms $\alpha, \beta,\delta$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transitivity:&lt;/strong&gt; Assume $\alpha \le_{HP} \beta$ and $\beta \le_{HP} \delta$. This means that $weight(\alpha) \le weight(\beta)$ and $weight(\beta) \le weight(\delta)$ which implies $weight(\alpha) \le weight(\delta)$ therefore giving $\alpha \le_{HP} \delta$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Totality:&lt;/strong&gt; For all $\alpha, \beta$ we have their weights, $weight(\alpha)$ and $weight(\beta)$. If $weight(\alpha) \le weight(\beta)$ then $\alpha \le_{HP} \beta$ and if $weight(\beta) \le weight(\alpha)$ then $\beta \le_{HP} \alpha$.
Therefore for all $\alpha, \beta$, we have $\alpha \le_{HP} \beta$ or $\beta \le_{HP} \alpha$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Minimality:&lt;/strong&gt; ($\Longrightarrow$) Assume T is consistent and $\alpha \notin T$. In the hierarchical weighting function we initially check if $\alpha \in T$. Since this is false, we assign $weight(\alpha) = -1$. The minimum weight any axiom $\beta$ can have is -1, therefore $weight(\alpha) \le weight(\beta)$ and $\alpha \le_{HP} \beta$ for all $\beta$.&lt;/p&gt;

&lt;p&gt;($\Longleftarrow$) Assume T is consistent and $\alpha \le_{HP} \beta$ for all $\beta$. We then get $weight(\alpha) \le weight(\beta)$ for all $\beta$. Since $T$ is consistent, we know that there exists some axiom $\delta \notin T$ that would make $T$ inconsistent, therefore $weight(\delta) = -1$. Since $weight(\alpha) \le weight(\beta)$ for all $\beta$ we must have $weight(\alpha)=-1$ which can only occur when $\alpha \notin T$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hierarchical:&lt;/strong&gt; Assume $\alpha \mapsto \beta$ and ${\alpha,\beta} \in T$. To check if $\alpha \le_{HP}\beta$ we begin by getting $weight(\alpha)$ and $weight(\beta)$. At the start of offset adjustment phase we have $weight(\alpha) = i$ and $weight(\beta) = j$ where $i,j \ge 0$. During the phase, offsets for $\alpha$ and $\beta$, $o_{\alpha}$ and $o_{\beta}$, are calculated and added to each of the axioms’ weights. Depending on the values of $i$ and $j$, $o_{\beta}$ has the value:&lt;/p&gt;

&lt;p&gt;       $\bullet$ If $i&amp;gt;j$ then $o_{\beta} = o_{\alpha}+i-j+1$&lt;/p&gt;

&lt;p&gt;       $\bullet$ If i $\le$ j then $o_{\beta} = o_{\alpha}$&lt;/p&gt;

&lt;p&gt;Applying these offsets, we get:&lt;/p&gt;

&lt;p&gt;       If $i&amp;gt;J$: &lt;br /&gt;        $weight(\alpha) = i + o_{\alpha}$ &lt;br /&gt;       $weight(\beta) = j + o_{\beta}$ &lt;br /&gt;                            $= j + o_{\alpha} +i - j + 1 $ &lt;br /&gt;                            $= i + o_{\alpha} +1$&lt;/p&gt;

&lt;p&gt;       If $i \le j$: &lt;br /&gt;       $weight(\alpha) = i + o_{\alpha}$ &lt;br /&gt;       $weight(\beta) = i + o_{\beta}$ &lt;br /&gt;                            $= i + o_{\alpha}$&lt;/p&gt;

&lt;p&gt;Therefore the hierarchical weighting function always terminates with $weight(\alpha) \le weight(\beta)$ and thus always returns $\alpha \le_{HP} \beta$ whenever $\alpha \mapsto \beta$ and ${\alpha, \beta} \in T$.&lt;/p&gt;

&lt;p&gt;This proves that the hierarchical preorder binary relation $\le_{HP}$ satisfies all four postulates.&lt;/p&gt;

&lt;h2 id=&quot;kernel-contraction-with-hierarchical-preorder&quot;&gt;Kernel Contraction with Hierarchical Preorder&lt;/h2&gt;

&lt;p&gt;Returning to the problem of selecting which hitting set to remove in the kernel contraction algorithm, we can extend the definition of $\le_{HP}$ to create a new total preorder relation that works with sets of axioms, $\le_{HPS}$. The relation $H_1 \le_{HPS} H_2$ where $H_1,H_2 \subseteq T$ states removing $H_2$ causes an equal or greater amount of epistemic lost as removing $H_1$. For set weights can say $weight(H_1) = sum(weight(\alpha))$ for all axioms $\alpha \in H_1$. To validate $\le_{HPS}$ relations we use the rule:&lt;/p&gt;

&lt;p&gt;       $H_1 \le_{HPS} H_2$ if $weight(H_1) &amp;lt; weight(H_2) $ where $H_1, H_2 \subseteq T$&lt;/p&gt;

&lt;p&gt;In order for the kernel contraction algorithm to be smooth, we must ensure that the same drop set is chosen every time we repeat a contraction. Therefore we assume that a canonical ordering of the axioms exists which can be used in tiebreakers where multiple axioms have the same minimum weight.&lt;/p&gt;

&lt;p&gt;When choosing which hitting set to remove, all of the hitting sets are ordered with $\le_{HPS}$ and then the set that is at the bottom of the preorder is selected as the drop set. Algorithm 2 outlines the kernel contraction algorithm $T\div\alpha$ using $\le_{HPS}$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/alg2.png&quot; alt=&quot;Algorithm 2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;smooth-kernel-contraction-proof&quot;&gt;Smooth Kernel Contraction Proof&lt;/h2&gt;

&lt;p&gt;We will now show that $T\div\alpha$ is a smooth kernel contraction by proving that all five postulates hold.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2:&lt;/strong&gt; The kernel contraction described in Algorithm 2 satisfies all five smooth kernel contraction postulates when applied an $\mathcal{EL^{++}}$ TBox.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Given a TBox $T$ and axioms ${\alpha, \beta}$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Success:&lt;/strong&gt; Assume $\nvdash\alpha$. When the pinpointing algorithm is applied, a finite number of $\alpha$-kernels are calculated that each entail $\alpha$. $\sigma(T \perp \alpha)$ is a minimum hitting set of the kernels so the set will include axioms from every kernel. Therefore the contracted TBox $T \setminus \sigma(T \perp \alpha)$ will have no $\alpha$-kernels and $T\div\alpha \nvdash \alpha$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inclusion:&lt;/strong&gt; The algorithm never adds new axioms to $T$ therefore $T\div\alpha \subseteq T$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Core Retainment:&lt;/strong&gt; Assume $\beta \in T$ and $\beta \notin T\div\alpha$. That means $\beta \in \sigma(T \perp \alpha)$. Set $T’=T-\alpha$ which means $T’ \subseteq T$ and $T’ \nvdash \alpha$. If we add $\beta$ to $T’$ then at least one $\alpha$-kernel exists in $T’ \cup \beta$ since $\beta$ is in a minimal hitting set of the kernels. Therefore $T’ \cup \beta \vdash \alpha$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Uniformity:&lt;/strong&gt; Assume for every $T’ \subseteq T$, we have $T’ \vdash \alpha$ iff $T’ \vdash \beta$. In [&lt;a href=&quot;#Hansson94&quot;&gt;Hansson94&lt;/a&gt;], it is explained that our assumption is equivalent to $T \perp \alpha = T \perp \beta$. When the incision function is run on
$T \perp \alpha$ and $T \perp \beta$, the same set of hitting sets will be calculated and since the preorder using
$\le_{HPS}$ is unique (assuming $&amp;lt;_*$ exists), $\sigma(T \perp \alpha) = \sigma(T \perp \beta)$ and therefore $T\div\alpha = T\div\beta$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relative Closure:&lt;/strong&gt; For some axiom $\beta$, if $\beta \in T$ and $\beta \in T\div\alpha$, then trivially $\beta \in T \cap Cn(T\div\alpha)$. Also trivially, if $\beta \notin T$ then $\beta \notin T \cap Cn(T\div\alpha)$.&lt;/p&gt;

&lt;p&gt;For the case where $\beta \in T$ and $\beta \notin T\div\alpha$. Let us assume that $\beta \in Cn(T\div\alpha)$. We know that $\beta \in  \sigma(T \perp \alpha)$ therefore $\beta$ is found in at least one $\alpha$-kernel. More specifically, since all hitting sets calculated are minimum, we know that for all $\alpha$-kernels that contain $\beta$, there must be some $\alpha$-kernel $k$ where no other $\delta \in \sigma(T \perp \alpha)$ is found. If this is not true then the hitting set would not be minimal since $\beta$ would be unnecessary. After removing the axioms of $\sigma(T \perp \alpha)$, we should have contracted $\alpha$ and have $T \perp \alpha = \emptyset$ however since $\beta \in Cn(T\div\alpha)$, there exists a subset of axioms $S \subseteq  T\div\alpha$ where $S \vdash \beta$ and therefore $S \cup k \setminus \beta \vdash \alpha$. This contradicts the success postulate for kernel contractions. Therefore
$\beta \notin Cn(T\div\alpha)$ thus $\beta \notin T \cap Cn(T\div\alpha)$. Since all axioms fall into one of these cases, we have $T \cap Cn(T\div\alpha) \subseteq T\div \alpha$ always holds.&lt;/p&gt;

&lt;p&gt;This proves that Algorithm 2 is a smooth kernel contraction.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;The hierarchical total preorder relation is a versatile method for ordering axioms and sets of axioms in TBoxes that do not need to be normalized and can be cyclic. When used in kernel contractions, the preorder limits the amount of epistemic loss caused to the TBox and maintains the smooth kernel contraction property.&lt;/p&gt;

&lt;p&gt;This hierarchical approach is simply one way of ordering axioms. Future work can be done into developing new preorders or epistemic entrenchments that use different approaches to weighting axioms that work better under certain contexts. An issue with the hierarchical weighting function currently is that after every contraction, the entire TBox needs to be re-weighted in order to update the preorder. Developing a way to adjust weights after a contraction is an avenue to explore
further. Finally, though the preorder was construction for $\mathcal{EL^{++}}$ TBoxes, it would be interesting to see if the method can be expanded to work with more expressive descriptions logics like the $\mathcal{ALC}$ family.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a name=&quot;Alchourron85&quot;&gt;&lt;/a&gt;[Alchourron85] Alchourrón, C., Gärdenfors, P., and Makinson, D.
(1985). On the logic of theory change: Partial meet contraction and revi-
sion functions. 50(2):510–530.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Baader05&quot;&gt;&lt;/a&gt;[Baader05] Baader, F., Brandt, S., and Lutz, C. (2005). Pushing the
EL envelope. In Proceedings of the Nineteenth International Joint Confer-
ence on Artificial Intelligence IJCAI-05, Edinburgh, UK. Morgan-Kaufmann
Publishers.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Baader07&quot;&gt;&lt;/a&gt;[Baader07] Baader, F., Calvanese, D., McGuiness, D., Nardi, D., and
Patel-Schneider, P., editors (2007). The Description Logic Handbook. Cam-
bridge University Press, Cambridge, second edition.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Baader08&quot;&gt;&lt;/a&gt;[Baader08] Baader, F. and Suntisrivaraporn, B.
(2008). Debugging SNOMED CT using axiom pinpointing in the description
logic EL+ . In Proceedings of the 3rd Knowledge Representation in Medicine
(KR-MED’08): Representing and Sharing Knowledge Using SNOMED, vol-
ume 410 of CEUR-WS.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Dawood17&quot;&gt;&lt;/a&gt;[Dawood17] Dawood, A., Delgrande, J., and Liao, Z. (2017). A study
of kernel contraction in EL. In Gordon, A., Miller, R., and Turan, G., editors,
Thirteenth International Symposium on Logical Formalizations of Common-
sense Reasoning, London, UK. (7 double-column AAAI-style pages).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Gardenfors88&quot;&gt;&lt;/a&gt;[Gardenfors88] Gärdenfors, P. and Makinson, D. (1988). Re-
visions of knowledge systems using epistemic entrenchment. In Proceedings
of the 2Nd Conference on Theoretical Aspects of Reasoning About Knowl-
edge, TARK ’88, pages 83–95, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Garey79&quot;&gt;&lt;/a&gt;[Garey79] Garey, M. and Johnson, D. (1979). Computers and
Intractability: A Guide to the Theory of NP-Completeness. W.H. Freeman
and Co., New York.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Hansson93&quot;&gt;&lt;/a&gt;[Hansson93] Hansson, S. O. (1993). Reversing the levi identity. 22(6):637–
669.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;Hansson94&quot;&gt;&lt;/a&gt;[Hansson94] Hansson, S. O. (1994). Kernel contraction. 59(3):845–859.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>